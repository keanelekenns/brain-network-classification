@inbook{lanciano2020,
    author = {Lanciano, Tommaso and Bonchi, Francesco and Gionis, Aristides},
    title = {Explainable Classification of Brain Networks via Contrast Subgraphs},
    year = {2020},
    isbn = {9781450379984},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3394486.3403383},
    abstract = {Mining human-brain networks to discover patterns that can be used to discriminate between healthy individuals and patients affected by some neurological disorder, is a fundamental task in neuro-science. Learning simple and interpretable models is as important as mere classification accuracy. In this paper we introduce a novel approach for classifying brain networks based on extracting contrast subgraphs, i.e., a set of vertices whose induced subgraphs are dense in one class of graphs and sparse in the other. We formally define the problem and present an algorithmic solution for extracting contrast subgraphs. We then apply our method to a brain-network dataset consisting of children affected by Autism Spectrum Disorder and children Typically Developed. Our analysis confirms the interestingness of the discovered patterns, which match background knowledge in the neuro-science literature. Further analysis on other classification tasks confirm the simplicity, soundness, and high explainability of our proposal, which also exhibits superior classification accuracy, to more complex state-of-the-art methods.},
    booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
    pages = {3308–3318},
    numpages = {11}
}

@techreport{goldberg1984,
    Author = {Goldberg, A. V.},
    Title = {Finding a Maximum Density Subgraph},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {1984},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/1984/5956.html},
    Number = {UCB/CSD-84-171},
    Abstract = {(none)}
}

@article{asahiro2000,
    title = {Greedily Finding a Dense Subgraph},
    journal = {Journal of Algorithms},
    volume = {34},
    number = {2},
    pages = {203-221},
    year = {2000},
    issn = {0196-6774},
    doi = {https://doi.org/10.1006/jagm.1999.1062},
    url = {https://www.sciencedirect.com/science/article/pii/S0196677499910623},
    author = {Yuichi Asahiro and Kazuo Iwama and Hisao Tamaki and Takeshi Tokuyama},
    abstract = {Given an n-vertex graph with nonnegative edge weights and a positive integer k≤n, our goal is to find a k-vertex subgraph with the maximum weight. We study the following greedy algorithm for this problem: repeatedly remove a vertex with the minimum weighted-degree in the currently remaining graph, until exactly k vertices are left. We derive tight bounds on the worst case approximation ratio R of this greedy algorithm: (1/2+n/2k)2−O(n−1/3)≤R≤(1/2+n/2k)2+O(1/n) for k in the range n/3≤k≤n and 2(n/k−1)−O(1/k)≤R≤2(n/k−1)+O(n/k2) for k<n/3. For k=n/2, for example, these bounds are 9/4±O(1/n), improving on naive lower and upper bounds of 2 and 4, respectively. The upper bound for general k compares well with currently the best (and much more complicated) approximation algorithm based on semidefinite programming.}
}

@inproceedings{tsourakakis2013,
    author = {Tsourakakis, Charalampos and Bonchi, Francesco and Gionis, Aristides and Gullo, Francesco and Tsiarli, Maria},
    title = {Denser than the Densest Subgraph: Extracting Optimal Quasi-Cliques with Quality Guarantees},
    year = {2013},
    isbn = {9781450321747},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.ezproxy.library.uvic.ca/10.1145/2487575.2487645},
    doi = {10.1145/2487575.2487645},
    abstract = {Finding dense subgraphs is an important graph-mining task with many applications. Given that the direct optimization of edge density is not meaningful, as even a single edge achieves maximum density, research has focused on optimizing alternative density functions. A very popular among such functions is the average degree, whose maximization leads to the well-known densest-subgraph notion. Surprisingly enough, however, densest subgraphs are typically large graphs, with small edge density and large diameter.In this paper, we define a novel density function, which gives subgraphs of much higher quality than densest subgraphs: the graphs found by our method are compact, dense, and with smaller diameter. We show that the proposed function can be derived from a general framework, which includes other important density functions as subcases and for which we show interesting general theoretical properties. To optimize the proposed function we provide an additive approximation algorithm and a local-search heuristic. Both algorithms are very efficient and scale well to large graphs.We evaluate our algorithms on real and synthetic datasets, and we also devise several application studies as variants of our original problem. When compared with the method that finds the subgraph of the largest average degree, our algorithms return denser subgraphs with smaller diameter. Finally, we discuss new interesting research directions that our problem leaves open.},
    booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {104–112},
    numpages = {9},
    keywords = {dense subgraph, graph mining, quasi-clique},
    location = {Chicago, Illinois, USA},
    series = {KDD '13}
}

@INPROCEEDINGS{yang2018,
    author={Yang, Yu and Chu, Lingyang and Zhang, Yanyan and Wang, Zhefeng and Pei, Jian and Chen, Enhong},
    booktitle={2018 IEEE 34th International Conference on Data Engineering (ICDE)},
    title={Mining Density Contrast Subgraphs},
    year={2018},
    volume={},
    number={},
    pages={221-232},
    doi={10.1109/ICDE.2018.00029}
}

@INPROCEEDINGS{cadena2016,
    author={Cadena, Jose and Vullikanti, Anil Kumar and Aggarwal, Charu C.},
    booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)}, 
    title={On Dense Subgraphs in Signed Network Streams}, 
    year={2016},
    volume={},
    number={},
    pages={51-60},
    doi={10.1109/ICDM.2016.0016}
}

@inproceedings{charikar2004,
    
    author={Charikar,M. and Wirth,A.},
    editor={ },
    year={2004},
    title={Maximizing quadratic programs: extending Grothendieck's inequality},
    publisher={IEEE},
    address={Los Alamitos CA},
    pages={54-60},
    abstract={This paper considers the following type of quadratic programming problem. Given an arbitrary matrix A, whose diagonal elements are zero, find x /spl isin/ {-1, 1}/sup n/ such that x/sup T/Ax is maximized. Our approximation algorithm for this problem uses the canonical semidefinite relaxation and returns a solution whose ratio to the optimum is in /spl Omega/(1/ logn). This quadratic programming problem can be seen as an extension to that of maximizing x/sup T/Ay (where y's components are also /spl plusmn/1). Grothendieck's inequality states that the ratio of the optimum value of the latter problem to the optimum of its canonical semidefinite relaxation is bounded below by a constant. The study of this type of quadratic program arose from a desire to approximate the maximum correlation in correlation clustering. Nothing substantive was known about this problem; we present an /spl Omega/ (1/logn) approximation, based on our quadratic programming algorithm. We can also guarantee that our quadratic programming algorithm returns a solution to the MAXCUT problem that has a significant advantage over a random assignment.},
    keywords={Algorithmics. Computability. Computer arithmetics; Applied sciences; Approximation algorithms; Clustering algorithms; Computer science; control theory; systems; Engineering profession; Exact sciences and technology; Linear matrix inequalities; Quadratic programming; Theoretical computing; US Department of Energy},
    isbn={0272-5428},
    language={English},
}


@inbook{dai2021,
    author = {Dai, Enyan and Wang, Suhang},
    title = {Towards Self-Explainable Graph Neural Network},
    year = {2021},
    isbn = {9781450384469},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3459637.3482306},
    abstract = {Graph Neural Networks (GNNs), which generalize the deep neural networks to graph-structured data, have achieved great success in modeling graphs. However, as an extension of deep learning for graphs, GNNs lack explainability, which largely limits their adoption in scenarios that demand the transparency of models. Though many efforts are taken to improve the explainability of deep learning, they mainly focus on i.i.d data, which cannot be directly applied to explain the predictions of GNNs because GNNs utilize both node features and graph topology to make predictions. There are only very few work on the explainability of GNNs and they focus on post-hoc explanations. Since post-hoc explanations are not directly obtained from the GNNs, they can be biased and misrepresent the true explanations. Therefore, in this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations. We propose a new framework which can find K-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.},
    booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
    pages = {302–311},
    numpages = {10}
}


@article{wang2021,
    author    = {Hao Wang and
               Yue Deng and
               Linyuan L{\"{u}} and
               Guanrong Chen},
    title     = {Hyperparameter-free and Explainable Whole Graph Embedding},
    journal   = {CoRR},
    volume    = {abs/2108.02113},
    year      = {2021},
    url       = {https://arxiv.org/abs/2108.02113},
    eprinttype = {arXiv},
    eprint    = {2108.02113},
    timestamp = {Thu, 05 Aug 2021 14:27:08 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-2108-02113.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gutierrez2019,
  title={Unsupervised network embeddings with node identity awareness},
  author={Guti{\'e}rrez-G{\'o}mez, Leonardo and Delvenne, Jean-Charles},
  journal={Applied Network Science},
  volume={4},
  number={1},
  pages={1--21},
  year={2019},
  publisher={SpringerOpen}
}

@article{elkan2012,
    title={Evaluating classifiers},
    author={Elkan, Charles},
    journal={San Diego: University of California},
    year={2012},
    publisher={Citeseer}
}

@article{hassan2021,
    author = {Hassan, Ali and Sulaiman, Riza and Abdulgabber, Mansoor and Kahtan, Hasan},
    year = {2021},
    month = {09},
    pages = {36-50},
    title = {TOWARDS USER-CENTRIC EXPLANATIONS FOR EXPLAINABLE MODELS: A REVIEW},
    volume = {6},
    journal = {Journal of Information System and Technology Management},
    doi = {10.35631/JISTM.622004}
}

@Article{linardatos2020,
    AUTHOR = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
    TITLE = {Explainable AI: A Review of Machine Learning Interpretability Methods},
    JOURNAL = {Entropy},
    VOLUME = {23},
    YEAR = {2021},
    NUMBER = {1},
    ARTICLE-NUMBER = {18},
    URL = {https://www.mdpi.com/1099-4300/23/1/18},
    PubMedID = {33375658},
    ISSN = {1099-4300},
    ABSTRACT = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into &ldquo;black box&rdquo; approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
    DOI = {10.3390/e23010018}
}

@article{goyal2018,
    title = {Graph embedding techniques, applications, and performance: A survey},
    journal = {Knowledge-Based Systems},
    volume = {151},
    pages = {78-94},
    year = {2018},
    issn = {0950-7051},
    doi = {https://doi.org/10.1016/j.knosys.2018.03.022},
    url = {https://www.sciencedirect.com/science/article/pii/S0950705118301540},
    author = {Palash Goyal and Emilio Ferrara},
    keywords = {Graph embedding techniques, Graph embedding applications, Python graph embedding methods GEM library},
    abstract = {Graphs, such as social networks, word co-occurrence networks, and communication networks, occur naturally in various real-world applications. Analyzing them yields insight into the structure of society, language, and different patterns of communication. Many approaches have been proposed to perform the analysis. Recently, methods which use the representation of graph nodes in vector space have gained traction from the research community. In this survey, we provide a comprehensive and structured analysis of various graph embedding techniques proposed in the literature. We first introduce the embedding task and its challenges such as scalability, choice of dimensionality, and features to be preserved, and their possible solutions. We then present three categories of approaches based on factorization methods, random walks, and deep learning, with examples of representative algorithms in each category and analysis of their performance on various tasks. We evaluate these state-of-the-art methods on a few common datasets and compare their performance against one another. Our analysis concludes by suggesting some potential applications and future directions. We finally present the open-source Python library we developed, named GEM (Graph Embedding Methods, available at https://github.com/palash1992/GEM), which provides all presented algorithms within a unified interface to foster and facilitate research on the topic.}
}

@article{craddock2013,
  title={The neuro bureau preprocessing initiative: open sharing of preprocessed neuroimaging data and derivatives},
  author={Craddock, Cameron and Benhajali, Yassine and Chu, Carlton and Chouinard, Francois and Evans, Alan and Jakab, Andr{\'a}s and Khundrakpam, Budhachandra Singh and Lewis, John David and Li, Qingyang and Milham, Michael and others},
  journal={Frontiers in Neuroinformatics},
  volume={7},
  year={2013}
}

@inproceedings{shang2018,
 author = {Shang, Jingbo and Shi, Xiyao and Jiang, Meng and Liu, Liyuan and Hanratty, Timothy and Han, Jiawei},
 booktitle = {arXiv:1802.06189 [cs]},
 month = {February},
 title = {Contrast Subgraph Mining from Coherent Cores},
 year = {2018}
}

@inproceedings{balalau2015,
author = {Balalau, Oana Denisa and Bonchi, Francesco and Chan, T-H. Hubert and Gullo, Francesco and Sozio, Mauro},
title = {Finding Subgraphs with Maximum Total Density and Limited Overlap},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684822.2685298},
doi = {10.1145/2684822.2685298},
abstract = {Finding dense subgraphs in large graphs is a key primitive in a variety of real-world application domains, encompassing social network analytics, event detection, biology, and finance. In most such applications, one typically aims at finding several (possibly overlapping) dense subgraphs which might correspond to communities in social networks or interesting events. While a large amount of work is devoted to finding a single densest subgraph, perhaps surprisingly, the problem of finding several dense subgraphs with limited overlap has not been studied in a principled way, to the best of our knowledge. In this work we define and study a natural generalization of the densest subgraph problem, where the main goal is to find at most $k$ subgraphs with maximum total aggregate density, while satisfying an upper bound on the pairwise Jaccard coefficient between the sets of nodes of the subgraphs. After showing that such a problem is NP-Hard, we devise an efficient algorithm that comes with provable guarantees in some cases of interest, as well as, an efficient practical heuristic. Our extensive evaluation on large real-world graphs confirms the efficiency and effectiveness of our algorithms.},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
pages = {379–388},
numpages = {10},
keywords = {dense subgraph, graph algorithms, graph mining},
location = {Shanghai, China},
series = {WSDM '15}
}

@Article{sewani2020,
    AUTHOR = {Sewani, Harshini and Kashef, Rasha},
    TITLE = {An Autoencoder-Based Deep Learning Classifier for Efficient Diagnosis of Autism},
    JOURNAL = {Children},
    VOLUME = {7},
    YEAR = {2020},
    NUMBER = {10},
    ARTICLE-NUMBER = {182},
    URL = {https://www.mdpi.com/2227-9067/7/10/182},
    PubMedID = {33066454},
    ISSN = {2227-9067},
    ABSTRACT = {Autism spectrum disorder (ASD) is a neurodevelopmental disorder characterized by a lack of social communication and social interaction. Autism is a mental disorder investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning models to enhance clinicians&rsquo; ability to provide robust diagnosis and prognosis of autism. However, with dynamic changes in autism behaviour patterns, these models&rsquo; quality and accuracy have become a great challenge for clinical practitioners. We applied a deep neural network learning on a large brain image dataset obtained from ABIDE (autism brain imaging data exchange) to provide an efficient diagnosis of ASD, especially for children. Our deep learning model combines unsupervised neural network learning, an autoencoder, and supervised deep learning using convolutional neural networks. Our proposed algorithm outperforms individual-based classifiers measured by various validations and assessment measures. Experimental results indicate that the autoencoder combined with the convolution neural networks provides the best performance by achieving 84.05% accuracy and Area under the Curve (AUC) value of 0.78.},
    DOI = {10.3390/children7100182}
}

@ARTICLE{thomas2020,
  
    AUTHOR={Thomas, Rajat Mani and Gallo, Selene and Cerliani, Leonardo and Zhutovsky, Paul and El-Gazzar, Ahmed and van Wingen, Guido},   
        
    TITLE={Classifying Autism Spectrum Disorder Using the Temporal Statistics of Resting-State Functional MRI Data With 3D Convolutional Neural Networks},      
        
    JOURNAL={Frontiers in Psychiatry},      
        
    VOLUME={11},           
        
    YEAR={2020},      
        
    URL={https://www.frontiersin.org/articles/10.3389/fpsyt.2020.00440},       
        
    DOI={10.3389/fpsyt.2020.00440},      
        
    ISSN={1664-0640},   
    
    ABSTRACT={Resting-state functional magnetic resonance imaging (rs-fMRI) data are 4-dimensional volumes (3-space + 1-time) that have been posited to reflect the underlying mechanisms of information exchange between brain regions, thus making it an attractive modality to develop diagnostic biomarkers of brain dysfunction. The enormous success of deep learning in computer vision has sparked recent interest in applying deep learning in neuroimaging. But the dimensionality of rs-fMRI data is too high (~20 M), making it difficult to meaningfully process the data in its raw form for deep learning experiments. It is currently not clear how the data should be engineered to optimally extract the time information, and whether combining different representations of time could provide better results. In this paper, we explored various transformations that retain the full spatial resolution by summarizing the temporal dimension of the rs-fMRI data, therefore making it possible to train a full three-dimensional convolutional neural network (3D-CNN) even on a moderately sized [~2,000 from Autism Brain Imaging Data Exchange (ABIDE)-I and II] data set. These transformations summarize the activity in each voxel of the rs-fMRI or that of the voxel and its neighbors to a single number. For each brain volume, we calculated regional homogeneity, the amplitude of low-frequency fluctuations, the fractional amplitude of low-frequency fluctuations, degree centrality, eigenvector centrality, local functional connectivity density, entropy, voxel-mirrored homotopic connectivity, and auto-correlation lag. We trained the 3D-CNN on a publically available autism dataset to classify the rs-fMRI images as being from individuals with autism spectrum disorder (ASD) or from healthy controls (CON) at an individual level. We attained results competitive on this task for a combined ABIDE-I and II datasets of ~66\%. When all summary measures were combined the result was still only as good as that of the best single measure which was regional homogeneity (ReHo). In addition, we also applied the support vector machine (SVM) algorithm on the same dataset and achieved comparable results, suggesting that 3D-CNNs could not learn additional information from these temporal transformations that were more useful to differentiate ASD from CON.}
}

@article{perotti2022,
  title={GRAPHSHAP: Motif-based Explanations for Black-box Graph Classifiers},
  author={Perotti, Alan and Bajardi, Paolo and Bonchi, Francesco and Panisson, Andr{\'e}},
  journal={arXiv preprint arXiv:2202.08815},
  year={2022}
}

@article{coupette2022, title={Differentially Describing Groups of Graphs}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/20312}, DOI={10.1609/aaai.v36i4.20312}, abstractNote={How does neural connectivity in autistic children differ from neural connectivity in healthy children or autistic youths? What patterns in global trade networks are shared across classes of goods, and how do these patterns change over time? Answering questions like these requires us to differentially describe groups of graphs: Given a set of graphs and a partition of these graphs into groups, discover what graphs in one group have in common, how they systematically differ from graphs in other groups, and how multiple groups of graphs are related. We refer to this task as graph group analysis, which seeks to describe similarities and differences between graph groups by means of statistically significant subgraphs. To perform graph group analysis, we introduce Gragra, which uses maximum entropy modeling to identify a non-redundant set of subgraphs with statistically significant associations to one or more graph groups. Through an extensive set of experiments on a wide range of synthetic and real-world graph groups, we confirm that Gragra works well in practice.}, number={4}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Coupette, Corinna and Dalleiger, Sebastian and Vreeken, Jilles}, year={2022}, month={Jun.}, pages={3959-3967} }

@Article{asan2020,
author="Asan, Onur
and Bayrak, Alparslan Emrah
and Choudhury, Avishek",
title="Artificial Intelligence and Human Trust in Healthcare: Focus on Clinicians",
journal="J Med Internet Res",
year="2020",
month="Jun",
day="19",
volume="22",
number="6",
pages="e15154",
keywords="human-AI collaboration; trust; technology adoption; FDA policy; bias; health care",
issn="1438-8871",
doi="10.2196/15154",
url="http://www.jmir.org/2020/6/e15154/",
url="https://doi.org/10.2196/15154",
url="http://www.ncbi.nlm.nih.gov/pubmed/32558657"
}

@misc{kahng2021,
  title={AI system outperforms humans in designing floorplans for microchips},
  author={Kahng, Andrew B},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{brzezicki2020,
title = {Artificial intelligence outperforms human students in conducting neurosurgical audits},
journal = {Clinical Neurology and Neurosurgery},
volume = {192},
pages = {105732},
year = {2020},
issn = {0303-8467},
doi = {https://doi.org/10.1016/j.clineuro.2020.105732},
url = {https://www.sciencedirect.com/science/article/pii/S0303846720300755},
author = {Maksymilian A. Brzezicki and Nicholas E. Bridger and Matthew D. Kobetić and Maciej Ostrowski and Waldemar Grabowski and Simran S. Gill and Sandra Neumann},
keywords = {Neurosurgery audits, Artificial intelligence, Computational neuroscience, Medical audits},
}

@article{fogel2018,
  title={Artificial intelligence powers digital medicine},
  author={Fogel, Alexander L and Kvedar, Joseph C},
  journal={NPJ digital medicine},
  volume={1},
  number={1},
  pages={1--4},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{grace2018,
  title={When will AI exceed human performance? Evidence from AI experts},
  author={Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang, Baobao and Evans, Owain},
  journal={Journal of Artificial Intelligence Research},
  volume={62},
  pages={729--754},
  year={2018}
}

@article{kong2019,
title = {Classification of autism spectrum disorder by combining brain connectivity and deep neural network classifier},
journal = {Neurocomputing},
volume = {324},
pages = {63-68},
year = {2019},
note = {Deep Learning for Biological/Clinical Data},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.04.080},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218306234},
author = {Yazhou Kong and Jianliang Gao and Yunpei Xu and Yi Pan and Jianxin Wang and Jin Liu},
keywords = {ASD, Morphological features, Individual network, Deep neural network, Classification},
abstract = {Autism spectrum disorder (ASD) is a common neurodevelopmental disorder that seriously affects communication and sociality of patients. It is crucial to accurately identify patients with ASD from typical controls (TC). Conventional methods for the classification of ASD/TC mainly extract morphological features independently at different regions of interest (ROIs), rarely considering the connectivity between these ROIs. In this study, we construct an individual brain network as feature representation, and use a deep neural network (DNN) classifier to perform ASD/TC classification. Firstly, we construct an individual brain network for each subject, and extract connectivity features between each pair of ROIs. Secondly, the connectivity features are ranked in descending order using F-score, and the top ranked features are selected. Finally, the selected 3000 top features are used to perform ASD/TC classification via a DNN classifier. An evaluation of the proposed method has been conducted with T1-weighted MRI images from the Autism Brain Imaging Data Exchange I (ABIDE I) by using ten-fold cross validation. Experimental results show that our proposed method can achieve the accuracy of 90.39\% and the area under receiver operating characteristic curve (AUC) of 0.9738 for ASD/TC classification. Comparison of experimental results illustrates that our proposed method outperforms some state-of-the-art methods in ASD/TC classification.}
}

@article{abbas2020,
  title={Multi-modular AI approach to streamline autism diagnosis in young children},
  author={Abbas, Halim and Garberson, Ford and Liu-Mayo, Stuart and Glover, Eric and Wall, Dennis P},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--8},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{knopf2021,
author = {Knopf, Alison},
title = {FDA authorizes marketing of diagnostic aid for Autism Spectrum Disorder},
journal = {The Brown University Child \& Adolescent Psychopharmacology Update},
volume = {23},
number = {8},
pages = {7-7},
doi = {https://doi.org/10.1002/cpu.30601},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpu.30601},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpu.30601},
abstract = {In June, the U.S. Food and Drug Administration authorized marketing of a device to help diagnose autism spectrum disorder (ASD). The Cognoa ASD Diagnosis Aid is a machine learning-based software intended to help health care providers diagnose ASD in children 18 months through 5 years of age who exhibit potential symptoms of the disorder.},
year = {2021}
}

@article{lord2006,
    author = {Lord, Catherine and Risi, Susan and DiLavore, Pamela S. and Shulman, Cory and Thurm, Audrey and Pickles, Andrew},
    title = "{Autism From 2 to 9 Years of Age}",
    journal = {Archives of General Psychiatry},
    volume = {63},
    number = {6},
    pages = {694-701},
    year = {2006},
    month = {06},
    abstract = "{Autism represents an unusual pattern of development beginning in the infant and toddler years.To examine the stability of autism spectrum diagnoses made at ages 2 through 9 years and identify features that predicted later diagnosis.Prospective study of diagnostic classifications from standardized instruments including a parent interview (Autism Diagnostic Interview–Revised [ADI-R]), an observational scale (Pre-Linguistic Autism Diagnostic Observation Schedule/Autism Diagnostic Observation Schedule [ADOS]), and independent clinical diagnoses made at ages 2 and 9 years compared with a clinical research team's criterion standard diagnoses.Three inception cohorts: consecutive referrals for autism assessment to (1) state-funded community autism centers, (2) a private university autism clinic, and (3) case controls with developmental delay from community clinics.At 2 years of age, 192 autism referrals and 22 developmentally delayed case controls; 172 children seen at 9 years of age.Consensus best-estimate diagnoses at 9 years of age.Percentage agreement between best-estimate diagnoses at 2 and 9 years of age was 67, with a weighted κ of 0.72. Diagnostic change was primarily accounted for by movement from pervasive developmental disorder not otherwise specified to autism. Each measure at age 2 years was strongly prognostic for autism at age 9 years, with odds ratios of 6.6 for parent interview, 6.8 for observation, and 12.8 for clinical judgment. Once verbal IQ (P = .001) was taken into account at age 2 years, the ADI-R repetitive domain (P = .02) and the ADOS social (P = .05) and repetitive domains (P = .005) significantly predicted autism at age 9 years.Diagnostic stability at age 9 years was very high for autism at age 2 years and less strong for pervasive developmental disorder not otherwise specified. Judgment of experienced clinicians, trained on standard instruments, consistently added to information available from parent interview and standardized observation.<!--
Arch Gen Psychiatry. 2006;63:694-701
-->}",
    issn = {0003-990X},
    doi = {10.1001/archpsyc.63.6.694},
    url = {https://doi.org/10.1001/archpsyc.63.6.694},
    eprint = {https://jamanetwork.com/journals/jamapsychiatry/articlepdf/209669/yoa50311.pdf},
}

@article{johnson2007,
  title={Identification and evaluation of children with autism spectrum disorders},
  author={Johnson, Chris Plauch{\'e} and Myers, Scott M and Council on Children with Disabilities},
  journal={Pediatrics},
  volume={120},
  number={5},
  pages={1183--1215},
  year={2007},
  publisher={American Academy of Pediatrics}
}

@book{dsm52013,
keywords = {Mental illness},
author = {APA},
language = {eng},
lccn = {2013011061},
title = {Diagnostic and statistical manual of mental disorders : DSM-5™.},
address = {Washington, DC ;},
edition = {5th edition.},
isbn = {9780890425541},
publisher = {American Psychiatric Publishing, a division of American Psychiatric Association},
year = {2013},
}

@article{grant2013,
  title={Proposed changes to the American Psychiatric Association diagnostic criteria for autism spectrum disorder: Implications for young children and their families},
  author={Grant, Roy and Nozyce, Molly},
  journal={Maternal and child health journal},
  volume={17},
  number={4},
  pages={586--592},
  year={2013},
  publisher={Springer}
}