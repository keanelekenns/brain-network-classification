@inbook{lanciano2020,
    author = {Lanciano, Tommaso and Bonchi, Francesco and Gionis, Aristides},
    title = {Explainable Classification of Brain Networks via Contrast Subgraphs},
    year = {2020},
    isbn = {9781450379984},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3394486.3403383},
    abstract = {Mining human-brain networks to discover patterns that can be used to discriminate between healthy individuals and patients affected by some neurological disorder, is a fundamental task in neuro-science. Learning simple and interpretable models is as important as mere classification accuracy. In this paper we introduce a novel approach for classifying brain networks based on extracting contrast subgraphs, i.e., a set of vertices whose induced subgraphs are dense in one class of graphs and sparse in the other. We formally define the problem and present an algorithmic solution for extracting contrast subgraphs. We then apply our method to a brain-network dataset consisting of children affected by Autism Spectrum Disorder and children Typically Developed. Our analysis confirms the interestingness of the discovered patterns, which match background knowledge in the neuro-science literature. Further analysis on other classification tasks confirm the simplicity, soundness, and high explainability of our proposal, which also exhibits superior classification accuracy, to more complex state-of-the-art methods.},
    booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
    pages = {3308–3318},
    numpages = {11}
}

@techreport{goldberg1984,
    Author = {Goldberg, A. V.},
    Title = {Finding a Maximum Density Subgraph},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {1984},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/1984/5956.html},
    Number = {UCB/CSD-84-171},
    Abstract = {(none)}
}

@article{asahiro2000,
    title = {Greedily Finding a Dense Subgraph},
    journal = {Journal of Algorithms},
    volume = {34},
    number = {2},
    pages = {203-221},
    year = {2000},
    issn = {0196-6774},
    doi = {https://doi.org/10.1006/jagm.1999.1062},
    url = {https://www.sciencedirect.com/science/article/pii/S0196677499910623},
    author = {Yuichi Asahiro and Kazuo Iwama and Hisao Tamaki and Takeshi Tokuyama},
    abstract = {Given an n-vertex graph with nonnegative edge weights and a positive integer k≤n, our goal is to find a k-vertex subgraph with the maximum weight. We study the following greedy algorithm for this problem: repeatedly remove a vertex with the minimum weighted-degree in the currently remaining graph, until exactly k vertices are left. We derive tight bounds on the worst case approximation ratio R of this greedy algorithm: (1/2+n/2k)2−O(n−1/3)≤R≤(1/2+n/2k)2+O(1/n) for k in the range n/3≤k≤n and 2(n/k−1)−O(1/k)≤R≤2(n/k−1)+O(n/k2) for k<n/3. For k=n/2, for example, these bounds are 9/4±O(1/n), improving on naive lower and upper bounds of 2 and 4, respectively. The upper bound for general k compares well with currently the best (and much more complicated) approximation algorithm based on semidefinite programming.}
}

@inproceedings{tsourakakis2013,
    author = {Tsourakakis, Charalampos and Bonchi, Francesco and Gionis, Aristides and Gullo, Francesco and Tsiarli, Maria},
    title = {Denser than the Densest Subgraph: Extracting Optimal Quasi-Cliques with Quality Guarantees},
    year = {2013},
    isbn = {9781450321747},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.ezproxy.library.uvic.ca/10.1145/2487575.2487645},
    doi = {10.1145/2487575.2487645},
    abstract = {Finding dense subgraphs is an important graph-mining task with many applications. Given that the direct optimization of edge density is not meaningful, as even a single edge achieves maximum density, research has focused on optimizing alternative density functions. A very popular among such functions is the average degree, whose maximization leads to the well-known densest-subgraph notion. Surprisingly enough, however, densest subgraphs are typically large graphs, with small edge density and large diameter.In this paper, we define a novel density function, which gives subgraphs of much higher quality than densest subgraphs: the graphs found by our method are compact, dense, and with smaller diameter. We show that the proposed function can be derived from a general framework, which includes other important density functions as subcases and for which we show interesting general theoretical properties. To optimize the proposed function we provide an additive approximation algorithm and a local-search heuristic. Both algorithms are very efficient and scale well to large graphs.We evaluate our algorithms on real and synthetic datasets, and we also devise several application studies as variants of our original problem. When compared with the method that finds the subgraph of the largest average degree, our algorithms return denser subgraphs with smaller diameter. Finally, we discuss new interesting research directions that our problem leaves open.},
    booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {104–112},
    numpages = {9},
    keywords = {dense subgraph, graph mining, quasi-clique},
    location = {Chicago, Illinois, USA},
    series = {KDD '13}
}

@INPROCEEDINGS{yang2018,
    author={Yang, Yu and Chu, Lingyang and Zhang, Yanyan and Wang, Zhefeng and Pei, Jian and Chen, Enhong},
    booktitle={2018 IEEE 34th International Conference on Data Engineering (ICDE)},
    title={Mining Density Contrast Subgraphs},
    year={2018},
    volume={},
    number={},
    pages={221-232},
    doi={10.1109/ICDE.2018.00029}
}

@INPROCEEDINGS{cadena2016,
    author={Cadena, Jose and Vullikanti, Anil Kumar and Aggarwal, Charu C.},
    booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)}, 
    title={On Dense Subgraphs in Signed Network Streams}, 
    year={2016},
    volume={},
    number={},
    pages={51-60},
    doi={10.1109/ICDM.2016.0016}
}

@inproceedings{charikar2004,
    
    author={Charikar,M. and Wirth,A.},
    editor={ },
    year={2004},
    title={Maximizing quadratic programs: extending Grothendieck's inequality},
    publisher={IEEE},
    address={Los Alamitos CA},
    pages={54-60},
    abstract={This paper considers the following type of quadratic programming problem. Given an arbitrary matrix A, whose diagonal elements are zero, find x /spl isin/ {-1, 1}/sup n/ such that x/sup T/Ax is maximized. Our approximation algorithm for this problem uses the canonical semidefinite relaxation and returns a solution whose ratio to the optimum is in /spl Omega/(1/ logn). This quadratic programming problem can be seen as an extension to that of maximizing x/sup T/Ay (where y's components are also /spl plusmn/1). Grothendieck's inequality states that the ratio of the optimum value of the latter problem to the optimum of its canonical semidefinite relaxation is bounded below by a constant. The study of this type of quadratic program arose from a desire to approximate the maximum correlation in correlation clustering. Nothing substantive was known about this problem; we present an /spl Omega/ (1/logn) approximation, based on our quadratic programming algorithm. We can also guarantee that our quadratic programming algorithm returns a solution to the MAXCUT problem that has a significant advantage over a random assignment.},
    keywords={Algorithmics. Computability. Computer arithmetics; Applied sciences; Approximation algorithms; Clustering algorithms; Computer science; control theory; systems; Engineering profession; Exact sciences and technology; Linear matrix inequalities; Quadratic programming; Theoretical computing; US Department of Energy},
    isbn={0272-5428},
    language={English},
}


@inbook{dai2021,
    author = {Dai, Enyan and Wang, Suhang},
    title = {Towards Self-Explainable Graph Neural Network},
    year = {2021},
    isbn = {9781450384469},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3459637.3482306},
    abstract = {Graph Neural Networks (GNNs), which generalize the deep neural networks to graph-structured data, have achieved great success in modeling graphs. However, as an extension of deep learning for graphs, GNNs lack explainability, which largely limits their adoption in scenarios that demand the transparency of models. Though many efforts are taken to improve the explainability of deep learning, they mainly focus on i.i.d data, which cannot be directly applied to explain the predictions of GNNs because GNNs utilize both node features and graph topology to make predictions. There are only very few work on the explainability of GNNs and they focus on post-hoc explanations. Since post-hoc explanations are not directly obtained from the GNNs, they can be biased and misrepresent the true explanations. Therefore, in this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations. We propose a new framework which can find K-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.},
    booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
    pages = {302–311},
    numpages = {10}
}


@article{wang2021,
    author    = {Hao Wang and
               Yue Deng and
               Linyuan L{\"{u}} and
               Guanrong Chen},
    title     = {Hyperparameter-free and Explainable Whole Graph Embedding},
    journal   = {CoRR},
    volume    = {abs/2108.02113},
    year      = {2021},
    url       = {https://arxiv.org/abs/2108.02113},
    eprinttype = {arXiv},
    eprint    = {2108.02113},
    timestamp = {Thu, 05 Aug 2021 14:27:08 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-2108-02113.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gutierrez2019,
  title={Unsupervised network embeddings with node identity awareness},
  author={Guti{\'e}rrez-G{\'o}mez, Leonardo and Delvenne, Jean-Charles},
  journal={Applied Network Science},
  volume={4},
  number={1},
  pages={1--21},
  year={2019},
  publisher={SpringerOpen}
}

@article{elkan2012,
    title={Evaluating classifiers},
    author={Elkan, Charles},
    journal={San Diego: University of California},
    year={2012},
    publisher={Citeseer}
}

@article{hassan2021,
    author = {Hassan, Ali and Sulaiman, Riza and Abdulgabber, Mansoor and Kahtan, Hasan},
    year = {2021},
    month = {09},
    pages = {36-50},
    title = {TOWARDS USER-CENTRIC EXPLANATIONS FOR EXPLAINABLE MODELS: A REVIEW},
    volume = {6},
    journal = {Journal of Information System and Technology Management},
    doi = {10.35631/JISTM.622004}
}

@Article{linardatos2020,
    AUTHOR = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
    TITLE = {Explainable AI: A Review of Machine Learning Interpretability Methods},
    JOURNAL = {Entropy},
    VOLUME = {23},
    YEAR = {2021},
    NUMBER = {1},
    ARTICLE-NUMBER = {18},
    URL = {https://www.mdpi.com/1099-4300/23/1/18},
    PubMedID = {33375658},
    ISSN = {1099-4300},
    ABSTRACT = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into &ldquo;black box&rdquo; approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
    DOI = {10.3390/e23010018}
}

@article{goyal2018,
    title = {Graph embedding techniques, applications, and performance: A survey},
    journal = {Knowledge-Based Systems},
    volume = {151},
    pages = {78-94},
    year = {2018},
    issn = {0950-7051},
    doi = {https://doi.org/10.1016/j.knosys.2018.03.022},
    url = {https://www.sciencedirect.com/science/article/pii/S0950705118301540},
    author = {Palash Goyal and Emilio Ferrara},
    keywords = {Graph embedding techniques, Graph embedding applications, Python graph embedding methods GEM library},
    abstract = {Graphs, such as social networks, word co-occurrence networks, and communication networks, occur naturally in various real-world applications. Analyzing them yields insight into the structure of society, language, and different patterns of communication. Many approaches have been proposed to perform the analysis. Recently, methods which use the representation of graph nodes in vector space have gained traction from the research community. In this survey, we provide a comprehensive and structured analysis of various graph embedding techniques proposed in the literature. We first introduce the embedding task and its challenges such as scalability, choice of dimensionality, and features to be preserved, and their possible solutions. We then present three categories of approaches based on factorization methods, random walks, and deep learning, with examples of representative algorithms in each category and analysis of their performance on various tasks. We evaluate these state-of-the-art methods on a few common datasets and compare their performance against one another. Our analysis concludes by suggesting some potential applications and future directions. We finally present the open-source Python library we developed, named GEM (Graph Embedding Methods, available at https://github.com/palash1992/GEM), which provides all presented algorithms within a unified interface to foster and facilitate research on the topic.}
}