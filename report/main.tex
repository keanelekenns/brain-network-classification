\documentclass[sigconf]{acmart}
% % Packages
\usepackage{soul}
% \usepackage{cite}
% \usepackage{balance}
% \usepackage{listings}
% \usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
% \usepackage{graphicx}
% \usepackage{textcomp}
% \usepackage{xcolor}
% \usepackage{booktabs}
% \usepackage{enumitem}
% \usepackage{hyperref}
% \usepackage{totpages}
% \usepackage{subcaption}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\begin{document}

% Specify path to images
\graphicspath{ {./img/} }


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Contrast Subgraphs of Brain Networks]{Improvements and Discussion of "Explainable Classification of Brain Networks via Contrast Subgraphs"}

\author{Tengkai Yu}
\email{FIXTHIS@uvic.ca}
\affiliation{%
  \institution{University of Victoria}
  \department{Computer Science}
  \streetaddress{PO Box 1700 STN CSC}
  \city{Victoria}
  \state{British Columbia}
  \country{Canada}
  \postcode{V8W 2Y2}
}

\author{Keanelek Enns}
\email{keanelekenns@uvic.ca}
\affiliation{%
  \institution{University of Victoria}
  \department{Computer Science}
  \streetaddress{PO Box 1700 STN CSC}
  \city{Victoria}
  \state{British Columbia}
  \country{Canada}
  \postcode{V8W 2Y2}
}

\author{Venkatesh Srinivasan}
\email{srinivas@uvic.ca}
\affiliation{%
  \institution{University of Victoria}
  \department{Computer Science}
  \streetaddress{PO Box 1700 STN CSC}
  \city{Victoria}
  \state{British Columbia}
  \country{Canada}
  \postcode{V8W 2Y2}
}

\author{Alex Thomo}
\email{thomo@uvic.ca}
\affiliation{%
  \institution{University of Victoria}
  \department{Computer Science}
  \streetaddress{PO Box 1700 STN CSC}
  \city{Victoria}
  \state{British Columbia}
  \country{Canada}
  \postcode{V8W 2Y2}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{T. Yu and K. Enns}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
\hl{TODO}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002950.10003624.10003633.10010917</concept_id>
       <concept_desc>Mathematics of computing~Graph algorithms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010444.10010446</concept_id>
       <concept_desc>Applied computing~Consumer health</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Graph algorithms}
\ccsdesc[300]{Applied computing~Consumer health}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{graph analytics, graph algorithms, brain networks, densest subgraph}

\settopmatter{printfolios=true} % for page numbering

%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction} \label{intro}

Graphs are used to represent a wide variety of complex entities, systems, and relationships.
As their use continues to grow in many areas of research and industry, graph classification, the process of labelling graphs as belonging to certain categories or classes, has become increasingly important \hl{find a citation to back up importance claim}.
One particularly interesting graph classification problem comes in the form of identifying individuals possessing certain neurological disorders by considering graphs that model functional connections in the brain (referred to as brain networks).

There are multiple ways to measure the quality of a graph classification model.
Metrics such as accuracy, precision, and recall are essential for evaluating any classifier \cite{elkan2012}, but recently there has been a trend towards explainability in the world of machine learning and artificial intelligence \cite{hassan2021,linardatos2020}.
This is because industries, governments, and organizations, especially those that deal with critical decision making such as the medical field, are hesitant to adopt prediction models without knowing \emph{how and why} they make decisions, regardless of how accurate these models are reported to be.
Therefore, it is increasingly important to find a balance between classification models that are explainable and simple to understand, while also achieving high accuracy, precision, and recall scores.

Explainability in classifiers is not only about building confidence in the user of the classifier.
Providing insights that may not have been detected through classical methods is another major benefit and may lead to further advancements in research.
These insights are especially helpful to neuroscientists when it comes to studying less understood neurological disorders such as Autism Spectrum Disorder (ASD).

The question at the core of this study is, \emph{"What information in a brain network can be used to discern between individuals with ASD and those typically developed (TD)?"}
Rather than plugging a group of brain networks into a black box classifier and asking it to predict the class of new brain networks, we would like to identify intuitive features of the networks so that we know \emph{why} a given network is being classified the way it is.
Thankfully, others have come before us with a similar goal.

In their paper, "Explainable Classification of Brain Networks via Contrast Subgraphs", Lanciano \emph{et al.} proposed a method for translating brain networks into two dimensional representations with a simple interpretation \cite{lanciano2020}.
This translation of a graph into a vector representation is known as a graph embedding and is fundamental to many graph classification problems \cite{goyal2018}.
The graph embedding employed by Lanciano \emph{et al.} involves the use of contrast subgraphs, which we define in section \ref{emb-technique}.

In this study, we sought to replicate the findings of Lanciano \emph{et al.} as well as discover alternatives or improvements to the methods described in their paper (referred to as the original paper from here on) \cite{lanciano2020}.
We found the quantitative results reported in the original paper were not reproducible, and much of the implementation was unavailable in the given replication package\footnote{The original paper's replication package can be found at \url{https://github.com/tlancian/contrast-subgraph} and was accessed in January 2022.}.
In light of this, we summarize our contributions as follows:

\begin{itemize}
    \item We provide a complete and optimized implementation of the original paper's graph embedding process\footnote{Our replication package can be found at \url{https://github.com/keanelekenns/contrast-subgraph}.}.
    \item We demonstrate various improvements to the original paper's methods such as 
    \begin{itemize}
        \item using a quadratic programming (QP) solver instead of a semidefinite programming (SDP) solver.
        \item optimizing the local search algorithm used.
        \item utilizing the top-k contrast subgraphs, as suggested by the original authors for future work.
    \end{itemize}
    \item We develop an entirely new graph embedding technique named Discriminative Edges (DE) that provides improved accuracy and reduced computational time over the original paper's methods, all while maintaining similar explainability.
    \item We provide code for evaluating the graph classification approaches along with recording specific parameters used and results associated with them.
\end{itemize}

The following section will discuss related works.
Section \ref{problem-statement} will give more context for the problem we are solving, along with a description of the solutions we plan to apply.
The process followed and the results obtained are outlined in section \ref{experiments}, and finally, we end the paper with discussions and conclusions for this study.

\section{Related Work} \label{related-work}

\hl{Are you supposed to mention the paper you are replicating in the related works? Also, to what extent should we mention the papers that Lanciano built off of?}

\hl{Talk about trend towards explainability in AI (xAI). A possible reference is https://doi.org/10.1016/j.inffus.2021.01.008}

Dai and Wang worked on creating explanations that are built into a GNN \cite{dai2021} rather than trying to find an explanation after the GNN had made its classification.
They found their method to be less biased and more computationally efficient when compared to using a post-hoc explainer such as GNN explainer.

Wang \emph{et al.} recently proposed a method, named DHC-E, for embedding graphs in a generalizable way that does not require any hyperparameters \cite{wang2021}. The embedding has a moderate number of dimensions depending on the graph set analyzed and is based on the h-index values of each vertex in the graph. They used principal component analysis to present the graph embeddings in two dimensions.

\hl{Look into https://doi.org/10.1142/S0129183118400077,

https://dl.acm.org/doi/abs/10.1145/3219819.3219980}

\section{Problem Statement} \label{problem-statement}

There are three main components for creating a graph classification model:

\begin{enumerate}
    \item Embedding technique: This is how the complex information of a graph is translated into a simplified representation. This representation often comes in the form of a real valued vector where each value is called a "feature".
    \item Classifier algorithm: This is the method of training the model to discern between (embedded) graphs of each class.
    \item Data: The instances (in this case, brain networks) that the model is trained with.
\end{enumerate}

\textbf{Embedding Technique.}
This paper focuses primarily on developing embedding techniques.
The techniques involve the use of contrast subgraphs and discriminative edges.
We give more thorough discussions of these techniques in the following sections.

\textbf{Classifier Algorithm.}
LinearSVC was chosen from the commonly used scikit-learn python library as a baseline classifier algorithm and was used for all the experiments.
This allows us to fairly compare the embedding techniques used.

\textbf{Data.}
A common group of data sets were used for the experiments.
Data from the Autism Brain Imaging Data Exchange (ABIDE) project \cite{craddock2013} was processed by Lanciano \emph{et al.}.
This processing resulted in groups of undirected, unweighted graphs (brain networks) defined over a common set of 116 vertices.
The vertices of the brain networks correspond to regions of interest (ROIs) in the brain, thus we are studying a special case of graph classification where all considered graphs possess the same set of vertices (i.e. regions of the brain which are, naturally, common to all brains).
The edges of each brain network correspond to functional (as opposed to structural) connections in the brain.
For a more thorough discussion of the data sets used and the processing that was performed by Lanciano \emph{et al.}, please see section 5 of the original paper \cite{lanciano2020}.

\subsection{Embedding Techniques} \label{emb-technique}
When it comes to creating an embedding technique in the context of brain network classification, the question we are asking is, \emph{"How can we translate a brain network into a vector/embedding in such a way that the embedding is intuitive \textbf{and} adequately separates different brain networks according to their classes?"}

The embedding must be intuitive in order for our classification model to be explainable and simple to understand.

The embedding must also separate the different classes of brain networks in order to help the classification algorithm learn the differences between them.
In the case of vector embeddings, this literally means that we want the vectors of each class to be as far apart from each other as they can be.

\textbf{Notation.}
We reuse the notation specified in the original paper and reiterate the important notation here.

Let the $i^{th}$ brain network be represented as an undirected, unweighted graph $G_i = (V, E_i)$, where $V$ is the common vertex set representing the 116 ROIs of the brain as discussed previously (i.e. $|V| = 116$) and $E_i$ is the set of edges belonging to $G_i$ (note: $E_i \subset V \times V$).

Let a \emph{summary graph} corresponding to a set of brain networks $\mathcal{A}$ be a weighted, undirected graph $G^{\mathcal{A}} = (V, w^{\mathcal{A}})$, where $w^{\mathcal{A}}: V \times V \rightarrow \mathbb{R}_+$ is a weight function that assigns a value to each pair of vertices in $V$.
For vertices $u,v \in V$, we define $w^{\mathcal{A}}(u,v)$ to be the fraction of networks in $\mathcal{A}$ that contain the edge $(u,v)$.


\subsubsection{Contrast Subgraph.}
A contrast subgraph is defined as a subset of vertices that induces a dense subgraph in one graph and a sparse subgraph in another, assuming that the graphs share a common vertex set.
The problem can be simplified to finding a set of vertices that induce a dense subgraph in the difference of the two graphs (in this case, summary graphs of each class).
We echo the problem definition of the original paper.

\emph{Problem 1 (Contrast Subgraph). Given two sets of observation graphs, i.e. the condition group $\mathcal{A} = \{G^{\mathcal{A}}_1, . . . , G^{\mathcal{A}}_{|\mathcal{A}|}\}$ and the control group $\mathcal{B} = \{G^{\mathcal{B}}_1, . . . , G^{\mathcal{B}}_{|\mathcal{B}|}\}$, and corresponding summary graphs $G^{\mathcal{A}} = (V, w^{\mathcal{A}})$ and $G^{\mathcal{B}} = (V, w^{\mathcal{B}})$, we seek to find a subset of vertices $S^* \subseteq V$ that maximizes the contrast subgraph objective}
\begin{align*}
    \delta (S) = \sum_{u,v \in S} \left(w^{\mathcal{A}}(u,v) - w^{\mathcal{B}}(u,v) - \alpha\right)
\end{align*}
\emph{where $\alpha \in \mathbb{R}_+$ is a user-defined parameter.}

The original paper also defines a symmetric variant of the contrast subgraph problem (\emph{Problem 2}) which shares the same definition of \emph{Problem 1}, but with an alternative objective

\begin{align*}
    \sigma (S) = \sum_{u,v \in S} \left(|w^{\mathcal{A}}(u,v) - w^{\mathcal{B}}(u,v)| - \alpha\right)
\end{align*}

The parameter $\alpha$ is used to penalize large contrast subgraphs.
It can be adjusted to vary the proportion of edges that are considered detrimental to the contrast subgraph objective.

As previously mentioned, we can simplify the problems by considering the difference network  $G^{\mathcal{A} - \mathcal{B}} = (V, w^{\mathcal{A} - \mathcal{B}})$, where \\ $w^{\mathcal{A} - \mathcal{B}}(u,v) = w^{\mathcal{A}}(u,v) - w^{\mathcal{B}}(u,v), \forall u,v \in V$.
In this context, finding a contrast subgraph is equivalent to finding a dense subgraph in the difference network (a dense subgraph is one that maximizes the appropriate objective function).

Contrast subgraphs can be used in the following ways to embed a graph.

\textbf{Problem 1.}
The original authors use contrast sugbraph overlap to create two features for each brain network.
Because the problem is asymmetric, two contrast subgraphs are obtained.
One is found using the difference network $G^{TD - ASD}$ (which is the result of subtracting the summary graph $G^{TD}$ from the summary graph $G^{ASD}$) and the other is found using $G^{ASD - TD}$.
Each feature corresponds to the number of edges in common between the brain network and each contrast subgraph.
\hl{reference figure representing problem 1}\ref{fig:prob1} 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, keepaspectratio=true]{test.png}
    \caption{\hl{Show a representative plot for problem 1}}
    \label{fig:prob1}
\end{figure}

\textbf{Problem 2.}
Only one contrast subgraph is obtained for this embedding technique.
The contrast subgraph is used to induce subgraphs in both of the summary graphs (i.e. $G^{TD}$ and $G^{ASD}$) as well as each individual brain network.
The distances, computed as the $L_1$ norms, from the induced brain network to each summary graph are then used as the two features for this embedding.
\hl{reference figure representing problem 2} \ref{fig:prob2}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, keepaspectratio=true]{test.png}
    \caption{\hl{Show a representative plot for problem 2}}
    \label{fig:prob2}
\end{figure}

\subsubsection{Discriminative Edges.}
In section 5.1 of the original paper, it was shown that the degrees of each vertex in the studied brain networks did not vary significantly between the two classes \cite{lanciano2020}, yet the contrast subgraphs are defined as a set of vertices, and every edge induced by these vertices is used for classification.
In light of this, we thought it would be beneficial to consider only the \emph{edges} that are important for distinguishing between the classes.

The discriminative edges (DE) embedding technique employs a very simple computation for calculating the similarity of a new brain network with each of the summary graphs for ASD and TD brain networks.
Initially, the idea was to identify the $n$ most positive edges and the $n$ most negative edges in the difference network $G^{ASD - TD}$.
These are the edges with the greatest disparity between the two classes (positive edges are more common in ASD brain networks, and negative edges are more common in TD brain networks), and are thus, the most discriminative edges.
For each new brain network $G_i$, we calculate the dot product of the n most positive edges in $G^{ASD - TD}$ with the corresponding edges of $G_i$ (in this case, $G_i$ is unweighted, so present edges have a value of 1, and edges that are not present have a value of 0, however, this calculation would still be effective if $G_i$ were weighted).
This gives $G_i$'s similarity with the ASD class.
A similar calculation is done for the negative edges to give the similarity with the TD class.
These similarities are used as the two features of the technique.

Surprisingly, this embedding technique did not show considerable improvement over the contrast subgraph technique in our initial experiments, so further improvements were made.

Rather than ignoring the values of edges in the difference network corresponding to edges that do not exist in $G_i$, we decided to make use of this information by scaling the edge values of $G_i$ such that nonexistent edges have a value of -1 and present edges continue to have a value of 1 (this scaling can also be applied in the case where $G_i$ is weighted).
This means that not having an edge among the most discriminative edges actually counts against the similarity score for each case.

Finally, we decided to add a third feature to the embedding technique.
This feature is derived in a very similar way, but uses every edge in the calculation, and thereby captures more information.
It is calculated by scaling the edge values of $G_i$ as discussed previously, multiplying each edge to its corresponding edge in the difference network, and summing the values.
This gives one score, a positive value implies similarity with the ASD class, whereas a negative value implies similarity with the TD class.
\hl{Note that we may need to change the order I mentioned here depending on what the figures show in terms of positive and negative scores} \ref{fig:DE}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, keepaspectratio=true]{test.png}
    \caption{\hl{Show a representative plot for DE}}
    \label{fig:DE}
\end{figure}


\section{Experiments} \label{experiments}

The data used for this study was briefly described in section \ref{problem-statement}.
Table \ref{tab:datasets} is a recreation of Table 1 in the original paper and gives information regarding the logical grouping of the studied individuals.
This will give context for the experiments presented in this section.

The following subsections correspond to some of the contributions discussed in section \ref{intro}.

\begin{table}%[]
    \centering
    \caption{An overview of the datasets used. The names of each dataset (first column) are indicative of the common traits of the subjects. This grouping was used to limit the impact of confounding factors in the study. The last two columns give the number of subjects belonging to the TD and ASD classes respectively.}
    \begin{tabular}{c c c c}
        \hline
        Dataset & Description & TD & ASD \\
        \hline
        children &  Age $\leq$ 9 & 52 & 49 \\
        adolescents &  Age $\in [15,20]$ & 121 & 116 \\
        eyesclosed &  Eyes closed during scanning & 158 & 136 \\
        male &  Male individuals & 418 & 420 \\
        \hline
    \end{tabular}
    \label{tab:datasets}
\end{table}

\subsection{Replication} \label{replication}

We began our experiments with an attempt to replicate the results of the original authors using the replication package provided (links to both the original authors' repository and our repository can be found in appendix \ref{artifacts}).
However, unfortunately, no code was provided (by the time of writing) for evaluating the classifier as discussed in section 5.1 of the original paper.
Furthermore, the code that was present for extracting a contrast subgraph was not conducive to large scale experiments.

With this knowledge, we decided to first rewrite and optimize the existing code such that it could be run repeatedly for our experiments.

The most notable inefficiency was the writing and reading of multiple files and the use of subprocess calls to multiple scripts written in Python and Matlab.
We replaced the Matlab SDP solver, which was reused from the work of Cadena \emph{et al.} \cite{cadena2016}, with a Python implementation using the CVXPY library\footnote{This library can be found at \url{https://www.cvxpy.org/}.}.
The solvers were tested to ensure that the same inputs produced the same outputs (so as to prevent any logical changes from this optimization).

Rather than passing data through the writing and reading of files and the use of subprocess calls to run new scripts, we modified the scripts to directly accept such data as arguments.
We then upgraded the code from Python 2.7 to 3.8 as well as removed unnecessary libraries.
After these changes, we obtained a logically equivalent implementation of the original code with increased computational efficiency.

However, only finding contrast subgraphs does not constitute the embedding technique.
These contrast subgraphs must be used to translate a brain network into a vector or point.
Because this translation was not present in the repository, we also created the necessary components for the embedding such as inducing subgraphs, counting overlap between contrast subgraphs and new brain networks, and calculating the $L_1$ norm as indicated by the original paper.

Though many components of the original scripts were modified, it is worth noting that many components were reused, and the modifications were tested to ensure logical equivalency.

After this, we developed a module for evaluating the embedding technique using five-fold cross validation.
Unfortunately, the results did not agree with what had been reported by the original authors.
This is discussed further in section \ref{results}.

\subsection{Improvements} \label{improvements}

Even after optimizations, the original contrast subgraph embedding technique used is quite slow, and the accuracy is not far above the baseline accuracy (we define this as the accuracy obtained when the most dominant class is always predicted).
Therefore we turned our attention to developing improvements for the contrast subgraph embedding technique.

\textbf{Quadratic Programming.}
We looked further into the work by Cadena \emph{et al.} which was repurposed by the original authors.
When creating a mathematical problem formulation for finding a dense subgraph in a signed network, Cadena \emph{et al.} propose a quadratic programming problem.
In their study, they choose to approximate the quadratic program with a semidefinite program and a rounding technique \cite{cadena2016}.
Lanciano \emph{et al.} then use this implementation to find a contrast subgraph in their context.

However, because this study aims to find a dense subgraph in a small difference network (116 by 116), it seemed reasonable to implement a QP solver to solve the problem directly.
We used a Python library named CVXOPT\footnote{This library can be found at \url{https://cvxopt.org/}}.
None of the QP solver libraries we found were able to restrict potential solution vectors to discrete values such as \{-1,1\} (i.e. out or in), so we restricted solution values to [-1,1] rounding positive values to 1 and non-positive values to -1.

The speed increase that resulted from changing the solver was significant as we will discuss in section \ref{results}.

\textbf{Local Search Optimization.}
The QP and SDP solvers only approximate the densest subgraph, so a local search step is used to improve the density of the chosen subgraph.
The localSearch algorithm developed by Tsourakakis \emph{et al.} \cite{tsourakakis2013} and used by both Cadena \emph{et al.} and the original authors works by considering the density of the subgraph if each vertex outside the subgraph were to be added to the subgraph, adding them when it increases the density.
This is repeated until all the vertices have been considered.
Afterwards, the algorithm seeks to remove a single vertex from the subgraph and does so if it finds one that increases the density.
This process is then repeated until a maximum number of iterations have been reached, or no vertices can be added or taken away from the subgraph to increase its density.
The pseudocode for the original algorithm can be found in the paper by Tsourakakis \emph{et al.} \cite{tsourakakis2013}.

Considering the goal of the local search algorithm is to increase the density of the subgraph, we made a modification to the algorithm such that it attempts to remove as many vertices as it can in each iteration (removing them only if they increase the density).
This modified local search algorithm can be seen in \hl{algorithm 1}, and was found to increase the accuracy of the classifiers that used it over the original implementation.

\textbf{Top-k Contrast Subgraphs.}
Finally, we follow the original authors' suggestion for future work and use the top-k contrast subgraphs.
For the sake of time, we did not implement it according to Balalau \emph{et al.} \cite{balalau2015} as suggested, but instead we took the approach described by Tsourakakis \emph{et al.} in section 4.1 of their paper \cite{tsourakakis2013} and simply removed the vertices of each contrast subgraph that was found and searched for a new contrast subgraph among the remaining vertices of the graph.

The resulting k vectors (or points) for each new brain network were added together to obtain the final embedding.


\subsection{Evaluation} \label{evaluation}

To ensure all techniques were evaluated fairly, we developed a common module to run the cross validation tests.
This required that each embedding technique conformed to a common interface:

\begin{itemize}
    \item \textbf{Inputs:} Training graphs, training labels, testing graphs, and any other inputs specific to the embedding technique.
    \item \textbf{Outputs:} Training points and testing points (the results of translating each brain network into a point/vector).
\end{itemize}

The testing module takes care of splitting the brain networks into training and testing sets.
After receiving the embedded graphs as points, it shifts the points such that the mean value of each dimension becomes zero, and it scales the points such that a standard deviation is one unit.
These points are then used to train a classifier with the training fold of the data, and the classifier is used to predict the class labels of the testing fold of the data.
Each iteration of the cross validation routine results in three plots being generated: one displays the training points according to their class labels, the next one displays the test points according to their class labels, and the third displays the test points according to the predictions that are made.
Furthermore, the number of folds to use is configurable, and there is an option to perform leave-one-out cross validation.
Finally, a cumulative confusion matrix is derived as the sum of the confusion matrices in each of the cross validation iterations and used to calculate the classifier's average accuracy, precision, recall, and f-1 score.

\subsection{Results} \label{results}

Various experiments were conducted during the course of this study.
However, due to the computation time required to evaluate our recreation of the original paper's method, it was not possible to compare results of more rigorous experiments (such as leave-one-out cross validation) for all methods.

Table \ref{tab:results} summarizes the results of the fourth round of experiments.
No tuning was performed in this round, however, tuning from the previous rounds was used to select the best parameter values for each method.
Five fold cross validation was performed for each method with an 80/20 train to test ratio.

To see the full results of all the experiment rounds, see the \emph{results} directory in this paper's repository (found in appendix \ref{artifacts} \hl{this folder does not exist in the repo at the moment}).

\begin{table*}[t]
    \centering
    \caption{Classifier Results. \hl{add more clarifying info}.}
    \begin{tabular}{c c c c c c c}
        \hline
        Dataset & Embedding Technique & Runtime (seconds) & Accuracy (\%) & Precision (\%) & Recall (\%) & F1-Score (\%)\\
        \hline
        children & CS-SDP-P1 & 201.9 & 61.39 & 69.23 & 61.02 & 64.86 \\
        baseline accuracy = 51.49\% & CS-SDP-P2 & 22.3 & 56.44 & 69.23 & 56.25 & 62.07 \\
        & CS-QP-P1 & 7.4 & 70.30 & 76.92 & 68.97 & 72.73 \\
        & CS-QP-P2 & 4.2 & 65.34 & 73.08 & 64.41 & 68.47 \\
        & DE & 3.9 & 62.38 & 75.00 & 60.94 & 67.24 \\
        \hline
        adolescents & CS-SDP-P1 & 1 & 1 & 1 & 1 & 1 \\
        baseline accuracy = 51.05\% & CS-SDP-P2 & 1 & 1 & 1 & 1 & 1 \\
        & CS-QP-P1 & 1 & 1 & 1 & 1 & 1 \\
        & CS-QP-P2 & 1 & 1 & 1 & 1 & 1 \\
        & DE & 1 & 1 & 1 & 1 & 1 \\
        \hline
        eyesclosed & CS-SDP-P1 & 1 & 1 & 1 & 1 & 1 \\
        baseline accuracy = 53.74\% & CS-SDP-P2 & 1 & 1 & 1 & 1 & 1 \\
        & CS-QP-P1 & 1 & 1 & 1 & 1 & 1 \\
        & CS-QP-P2 & 1 & 1 & 1 & 1 & 1 \\
        & DE & 1 & 1 & 1 & 1 & 1 \\
        \hline
        male & CS-SDP-P1 & 1 & 1 & 1 & 1 & 1 \\
        baseline accuracy = 50.12\% & CS-SDP-P2 & 1 & 1 & 1 & 1 & 1 \\
        & CS-QP-P1 & 1 & 1 & 1 & 1 & 1 \\
        & CS-QP-P2 & 1 & 1 & 1 & 1 & 1 \\
        & DE & 1 & 1 & 1 & 1 & 1 \\
        \hline
    \end{tabular}
    \label{tab:results}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, keepaspectratio=true]{test.png}
    \caption{\hl{This is just a test image, we need to create proper images with proper labels}}
    \label{fig:my_label}
\end{figure}


\section{Discussion} \label{discussion}

Lanciano \emph{et al.} showed that the degrees of each vertex in the studied brain networks did not vary significantly between the two classes \cite{lanciano2020},
yet the contrast subgraphs are defined as a set of vertices, and every edge induced by these vertices is used for classification.
Instead, we should look solely at edges that are important for distinguishing between the classes.

The discriminative edge method considers the most important edges in each class (defined as having the most positive and most negative weights in the difference network) when creating an embedding.
Additionally, it considers the brain network as a whole in its third dimension, which likely captures more interesting information with respect to complex connections in the brain.
However, it does not specifically identify higher order structures in the brain networks that may be leading to the DNN's success.

It was observed in a number of experiments that the local search algorithm would find a subgraph that was significantly different than (sometimes entirely disjoint from) the subgraph found by the SDP solver despite using the SDP solver's solution as a starting point.
This seems to imply that the local search algorithm may have done most of the work in finding the densest subgraph, and that the SDP solving step may have been extraneous, though further investigation would be required to verify this.

\textbf{Limitations and Threats to Validity.}
This study was limited primarily by the data set used.
In many machine learning applications, a large volume of data is needed to adequately train and test new models.
With a small data set, it is difficult to know how generalizable any of the discussed methods are to new brain networks that we wish to classify.
Additionally, the way the data set was created and the individuals that were studied have a large impact on the validity of the classification methods developed.
Elkan gave a helpful discussion of potential pitfalls regarding evaluating classifiers and the way data is gathered \cite{elkan2012}.
In this case, the limitations in data set size and quantity are due to the costly nature of creating such brain networks as individuals need to undergo a series of scans and time series of these scans are used to create the resultant brain network \cite{lanciano2020}.
Furthermore, the ratio of the studied classes is imbalanced when compared to the true ratio of individuals with ASD.

Autism Spectrum Disorder is a very complex disorder with unclear definitions, and it may manifest in various different ways [\hl{perhaps this belongs in the intro when describing why the problem is hard?}].
Above all this, the brain is a highly complex organ that is not fully understood.
The brains of individuals are unique and develop in complex manners.
There may be a plethora of confounding factors that have an effect on the classification of such networks with respect to identifying ASD in individuals.

\section{Conclusion} \label{conclusion}

\textbf{Future Work.}
Future work should attempt to identify higher order structures (such as triangles, k-clusters, or graphlets of varying size and shapes) as a means of embedding.
This could be done by identifying prominent structures in each summary graph (perhaps after thresholding the edge weights), eliminating structures common to both, and counting the number of overlapping structures that a new brain network has in common with each class.
The challenge associated with this technique comes with the computational complexity of enumerating such high order structures.

As discussed in section \ref{discussion}, this study should be replicated on additional data sets to further test its validity.

\bibliographystyle{ACM-Reference-Format}
\bibliography{refs.bib}

\appendix
\section*{APPENDICES}
\section{Artifacts} \label{artifacts}

Original Paper's Repository:\\
\url{https://github.com/tlancian/contrast-subgraph}

Our Paper's Repository:\\
\url{https://github.com/keanelekenns/contrast-subgraph}

\end{document}
\endinput